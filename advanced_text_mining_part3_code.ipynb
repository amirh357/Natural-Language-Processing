{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## ADVANCED TEXT MINING PART3 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 3: Import packages  ####\n",
    "\n",
    "# Helper packages.\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Packages for loading pre-trained word-embedding model\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 4: Directory settings  ####\n",
    "\n",
    "from pathlib import Path\n",
    "# Set `home_dir` to the root directory of your computer.\n",
    "home_dir = Path.home()\n",
    "\n",
    "# Set `main_dir` to the location of your `booz-allen-hamilton` folder.\n",
    "main_dir = home_dir / \"Desktop\" / \"booz-allen-hamilton\"\n",
    "\n",
    "# Make `data_dir` from the `main_dir` and remainder of the path to data directory.\n",
    "data_dir = main_dir / \"data\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 5: Working directory  ####\n",
    "\n",
    "# Set working directory.\n",
    "os.chdir(data_dir)\n",
    "# Check working directory.\n",
    "print(os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 6: Loading text data  ####\n",
    "\n",
    "# Load corpus from a csv (for Mac).\n",
    "NYT = pd.read_csv('NYT_article_data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 7: Recap: corpus pre-processing steps  ####\n",
    "\n",
    "num_docs = len(NYT[\"snippet\"])\n",
    "print(num_docs)\n",
    "\n",
    "# Tokenize each text into a large list of tokenized snippets.\n",
    "NYT_tokenized = [word_tokenize(snippet) for snippet in NYT[\"snippet\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 8: Cleaning function  ####\n",
    "\n",
    "def prep_text(text_tokenized):\n",
    "    # Process words in all snippets.\n",
    "    clean_text = [None]*len(text_tokenized)\n",
    "\n",
    "    for i in range(len(text_tokenized)):\n",
    "        # 1. Convert to lower case.\n",
    "        text = [word.lower() for word in text_tokenized[i]]\n",
    "        # 2. Remove stop words.\n",
    "        text = [word for word in text if not word in stop_words]\n",
    "        # 3. Remove punctuation and any non-alphabetical characters.\n",
    "        text = [word for word in text if word.isalpha()]\n",
    "        clean_text[i] = text\n",
    "\n",
    "    clean_text_list = [' '.join(snippet) for snippet in clean_text]\n",
    "\n",
    "    return clean_text_list, clean_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 9: Prep NYT text for analysis  ####\n",
    "\n",
    "NYT_clean_list, NYT_clean = prep_text(NYT_tokenized)\n",
    "print(NYT_clean[:3])\n",
    "print(NYT_clean_list[:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 10: Recap: create a DTM  ####\n",
    "\n",
    "# Initialize `CountVectorizer`.\n",
    "vec = CountVectorizer()\n",
    "\n",
    "# Transform the list of snippets into DTM.\n",
    "X = vec.fit_transform(NYT_clean_list)\n",
    "print(X.toarray()) #<- show output as a matrix\n",
    "\n",
    "print(vec.get_feature_names()[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 11: Recap: create a DTM (cont'd)  ####\n",
    "\n",
    "# Convert the matrix into a pandas dataframe for easier manipulation.\n",
    "DTM_not_stemmed = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\n",
    "print(DTM_not_stemmed.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 22: gensim.models.Word2Vec  ####\n",
    "\n",
    "model = Word2Vec(NYT_clean, size = 50, min_count = 3, iter = 15, seed = 2)\n",
    "print(model.vector_size)\n",
    "\n",
    "print(NYT_clean[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 23: Word2Vec: most similar words  ####\n",
    "\n",
    "print(model.wv.most_similar('government'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 24: Word2Vec: most similar words  ####\n",
    "\n",
    "print(model.wv.most_similar('trade', topn = 5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 26: Exercise 1  ####\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 33: Load GloVe text files  ####\n",
    "\n",
    "# Number of glove dimensions.\n",
    "GLOVE_DIM = 200\n",
    "\n",
    "# Load pre-trained glove embeddings.\n",
    "glove_file = data_dir + \"/glove.6B.200d.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 34: Load GloVe text files  ####\n",
    "\n",
    "def LoadGloveModel(glove_file):\n",
    "    print(\"Loading GloVe Model\")\n",
    "    f = open(glove_file,'r',encoding=\"utf8\")\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\", len(model), \" words loaded!\")\n",
    "    return model\n",
    "\n",
    "# Load embeddings from file.\n",
    "glove_model = LoadGloveModel(glove_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 35: Load GloVe text files  ####\n",
    "\n",
    "dict(list(glove_model.items())[0:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 37: Word count of corpus  ####\n",
    "\n",
    "# Save series as a dictionary.\n",
    "corpus_freq_dist = DTM_not_stemmed.sum(axis = 0).to_dict()\n",
    "dict(list(corpus_freq_dist.items())[0:5])\n",
    "\n",
    "# Extract word counts for exploratory analysis.\n",
    "word_counts = pd.DataFrame(list(corpus_freq_dist.items()), columns = ['word', 'count'])\n",
    "print(word_counts.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 38: Word embeddings matrix  ####\n",
    "\n",
    "# Initialize embeddings matrix.\n",
    "DICT_SIZE = len(word_counts.index)\n",
    "word_emb_matrix = np.zeros((DICT_SIZE, GLOVE_DIM))\n",
    "words = list(word_counts.word)\n",
    "NUM_MESSAGES = len(NYT_tokenized)\n",
    "\n",
    "for i in range(DICT_SIZE):\n",
    "    w = words[i]\n",
    "    vect = glove_model.get(w)\n",
    "\n",
    "    if vect is not None:\n",
    "        word_emb_matrix[i] = vect\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 39: Word embeddings matrix  ####\n",
    "\n",
    "print(word_emb_matrix.shape)\n",
    "print(word_emb_matrix[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 40: NYT Article embeddings matrix  ####\n",
    "\n",
    "# Convert dataframe to a numpy array.\n",
    "DTM_not_stemmed = DTM_not_stemmed.to_numpy()\n",
    "\n",
    "# Compute sums of all word counts for each chat message.\n",
    "DTM_row_sums = np.sum(DTM_not_stemmed, axis=1)\n",
    "\n",
    "NYT_embeddings_matrix = DTM_not_stemmed.dot(word_emb_matrix)\n",
    "print(DTM_not_stemmed.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 41: NYT Article embeddings  ####\n",
    "\n",
    "for i in range(NUM_MESSAGES):\n",
    "    NYT_embeddings_matrix[i] = np.true_divide(NYT_embeddings_matrix[i], DTM_row_sums[i])\n",
    "\n",
    "# Save as a dataframe and add NYT snippet IDs.\n",
    "NYT_emb_df = pd.DataFrame(NYT_embeddings_matrix)\n",
    "\n",
    "print(NYT_emb_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 43: Exercise 2  ####\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 49: Find documents similar to snippet 25  ####\n",
    "\n",
    "NYT_snippets = NYT['snippet']\n",
    "NYT_snippets[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 50: Find documents similar to snippet 25  ####\n",
    "\n",
    "# Average embeddings.\n",
    "target_NYT_emb = NYT_emb_df.loc[1].to_numpy()\n",
    "target_NYT_emb[0:5]\n",
    "target_NYT_emb.reshape(1, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 51: Compute cosine similarity   ####\n",
    "\n",
    "similarity_scores = cosine_similarity(NYT_emb_df, target_NYT_emb.reshape(1, -1))\n",
    "similarity_scores[0:5]\n",
    "\n",
    "similarity_scores_df = pd.DataFrame(similarity_scores,\n",
    "                                    columns = ['similarity_score'],\n",
    "                                    index = NYT.index)\n",
    "\n",
    "print(similarity_scores_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 52: View results  ####\n",
    "\n",
    "similarity_scores_df.sort_values('similarity_score', ascending = False).head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 53: View results  ####\n",
    "\n",
    "print(NYT_snippets[168])\n",
    "print(NYT_snippets[112])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 54: Cosine similarity score distribution plot  ####\n",
    "\n",
    "# Plot results.\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "cm = plt.cm.PRGn\n",
    "n, bins, patches = plt.hist(similarity_scores_df['similarity_score'],\n",
    "20, color='green')\n",
    "\n",
    "for i, p in enumerate(patches):\n",
    "    plt.setp(p, 'facecolor', cm(i/25)) # notice the i/25\n",
    "    fig.suptitle('Distribution of Cosine Similarity Scores', fontsize=20)\n",
    "    plt.xlabel(\"Similarity score\", fontsize=18)\n",
    "    plt.ylabel(\"Number of NYT snippets\", fontsize=18)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 56: Exercise 3  ####\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
