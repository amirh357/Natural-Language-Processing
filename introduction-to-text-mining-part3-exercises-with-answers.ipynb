{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Mining Part 3 - Exercises with answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Load libraries that are used in this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper packages.\n",
    "import os \n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cosine similarity and clustering packages.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster.hierarchy import ward, dendrogram, fcluster\n",
    "import gensim\n",
    "from gensim import matutils\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Network creation and visualization.\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Other plotting tools.\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2 \n",
    "##### Set `main_dir` to the location of your `booz-allen-hamilton` folder.\n",
    "##### Make `data_dir` from the `main_dir` and concatenate remainder of the path to data directory.\n",
    "##### Make `plot_dir` from the `main_dir` and concatenate remainder of the path to plots directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Set `home_dir` to the root directory of your computer.\n",
    "home_dir = Path.home()\n",
    "\n",
    "# Set `main_dir` to the location of your `booz-allen-hamilton` folder.\n",
    "main_dir = home_dir / \"Desktop\" / \"booz-allen-hamilton\"\n",
    "\n",
    "# Make `data_dir` from the `main_dir` and remainder of the path to data directory.\n",
    "data_dir = main_dir / \"data\"\n",
    "\n",
    "# Make `plot_dir` from the `main_dir` and remainder of the path to data directory.\n",
    "plot_dir = main_dir / \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3 \n",
    "##### Set the working directory to `data_dir`.\n",
    "##### Check if the working directory is updated to `data_dir`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory.\n",
    "os.chdir(data_dir)\n",
    "\n",
    "# Check the working directory.\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 4\n",
    "##### Load the pickled file from the previous exercises: \n",
    "\n",
    "##### 'dictionary_ex.sav', 'corpus_tfidf_ex.sav', 'bow_corpus_ex.sav', 'ex_DTM.sav' and 'ex_titles_clean.sav' and name them as\n",
    "##### 'dictionary_ex', 'corpus_tfidf_ex', 'bow_corpus_ex', 'ex_DTM' and 'processed_docs_ex'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_ex = pickle.load(open('dictionary_ex.sav', 'rb'))\n",
    "corpus_tfidf_ex = pickle.load(open('corpus_tfidf_ex.sav', 'rb'))\n",
    "bow_corpus_ex = pickle.load(open('bow_corpus_ex.sav', 'rb'))\n",
    "DTM_ex = pickle.load(open('ex_DTM.sav', 'rb'))\n",
    "processed_docs_ex = pickle.load(open('ex_titles_clean.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 5 \n",
    "##### Load UN agreement titles data from original file, 'UN_agreement_titles.csv'.\n",
    "##### Load pre-saved word counts array we pickled in the previous session, `ex_word_counts_array.sav`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UN = pd.read_csv('UN_agreement_titles.csv')\n",
    "\n",
    "word_counts_array_ex = pickle.load(open(\"ex_word_counts_array.sav\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Now we are going to run LDA on our `corpus_tfidf_ex` object.\n",
    "##### Choose the same parameters as we did in the slides.\n",
    "##### Save the model as `lda_model_tfidf_ex` and print."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf_ex = gensim.models.LdaModel(corpus_tfidf_ex, num_topics = 5, \n",
    "                                                id2word = dictionary_ex, passes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model_tfidf_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "##### Look at the output of your LDA model, print each of the 5 topics and the top words within each topic.\n",
    "##### Then, take the first doc from `processed_docs_ex` and classify it within one of the five topics. Which one is it most similar to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at all 5 topics and top words within each one.\n",
    "for idx, topic in lda_model_tfidf_ex.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the first doc in processed_docs_ex.\n",
    "for index, score in sorted(lda_model_tfidf_ex[corpus_tfidf_ex[1]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf_ex.print_topic(index, 10)))\n",
    "\n",
    "# Topic 1 is the closest to title 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 \n",
    "##### Find the topic coherence for the LDA model.\n",
    "##### Save it as `coherence_lda_ex` and print it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = CoherenceModel(model = lda_model_tfidf_ex, texts = processed_docs_ex, \n",
    "                                     dictionary = dictionary_ex, coherence = 'c_v')\n",
    "coherence_lda_ex = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2 \n",
    "##### Define the convenience function `compute_coherence_values` and tweak parameters as you think needed.\n",
    "##### Set the seed to 1.\n",
    "##### Compute the `model_list` and `coherence_values` using the function above.\n",
    "##### There are more titles, so maybe more topics will be necessary as a limit. Hence, set `limit` as `80`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function defined in class.\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start = 2, step = 3):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.LdaMulticore(corpus = corpus, id2word = dictionary, num_topics = num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model = model, texts = texts, dictionary = dictionary, coherence = 'c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "model_list, coherence_values = compute_coherence_values(dictionary = dictionary_ex, corpus = corpus_tfidf_ex, \n",
    "                                                        texts = processed_docs_ex, start = 2, limit = 80, step = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3 \n",
    "##### Plot the findings from `model_list` and `coherence_values`.\n",
    "##### Set `x` as the range where `start` is `2`, `limit` is `80` and `step` `6`.\n",
    "##### Plot `x` against `coherence_values`. Label the axes accordingly.\n",
    "##### What would improve our LDA model? What number of topics make the most sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimal number of topics.\n",
    "import matplotlib.pyplot as plt\n",
    "limit = 80; start = 2; step = 6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc = 'best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It looks like the optimal topic is either around 30 or around 70."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 4\n",
    "##### Prepare the visualization object for plotting, and assign it to a new variable 'vis'.\n",
    "###### Display the results.\n",
    "##### What can you infer about the topics by looking at this plot?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adjust the slider from 0 to 1. What can you tell about the relevant terms in topic 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LDA vis object by providing:\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_tfidf_ex, #<- model object\n",
    "                              corpus_tfidf_ex,    #<- corpus object\n",
    "                              dictionary_ex)      #<- dictionary object\n",
    "\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference about the topics:**\n",
    "Based on distances between the circles, topics `1` and `2` seem to be the most similar in meaning.\n",
    "Based on circle sizes, topic `1` has the maximum number of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relevant terms in topic 3:** `Amend`, `Convent` and `protocol` seem to be the top most relevant terms in topic 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 \n",
    "##### Obtain the topic probabilities for the 1st document in our corpus.\n",
    "##### Which topic has the highest probability for document 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the index of the document in corpus.\n",
    "doc_num = 0\n",
    "\n",
    "# Extract the vector of tf_idf weights for the document.\n",
    "doc_vec = corpus_tfidf_ex[doc_num]\n",
    "print(doc_vec)\n",
    "\n",
    "# Extract topic probabilities for that document. \n",
    "# Note: The probability values may change for different seed values\n",
    "doc_topics = lda_model_tfidf_ex.get_document_topics(doc_vec)\n",
    "print(doc_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic `2` has the highest probability for document `1`. (remember, it's listed as `1` because the index starts at `0`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2 \n",
    "##### Now get the best topic and its probability for the document programmatically.\n",
    "##### What is the best topic and its probability in document 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed.\n",
    "np.random.seed(2)\n",
    "# Initialize maximum probability score.\n",
    "max_prob = 0\n",
    "\n",
    "# Initialize best topic.\n",
    "best_topic = 0\n",
    "\n",
    "# Loop over all topics for the document.\n",
    "for topic in doc_topics:\n",
    "    if max_prob <= topic[1]:  #<- if current topic's probability is as high as max\n",
    "        max_prob = topic[1]   #<- make current topic's probability the new max\n",
    "        best_topic = topic[0] #<- make current topic best\n",
    "\n",
    "# Create a tuple with information we just got.\n",
    "doc_topic_pair_ex = (doc_num, best_topic, max_prob)\n",
    "print(doc_topic_pair_ex)\n",
    "\n",
    "# We can see that for document 1, the best topic is 2 and its probability is 76.1%\n",
    "# Note: the probability values may change for different seed values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3 \n",
    "##### Define a function, GetDocTopicPair(), to extract this information for a document given an LDA model.\n",
    "\n",
    "##### Put it all together into a function that returns a tuple with the index of the document, the best fit topic, and its probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDocTopicPair(doc_num, corpus, lda_model_tfidf):\n",
    "# Extract the vector of tf_idf weights for the document.    \n",
    "    doc_vec = corpus[doc_num]\n",
    "# Extract topic probabilities for that document.\n",
    "    doc_topics = lda_model_tfidf.get_document_topics(doc_vec)\n",
    "    max_prob = 0\n",
    "    best_topic = 0\n",
    "    for topic in doc_topics:\n",
    "        if max_prob <= topic[1]:\n",
    "            max_prob = topic[1]\n",
    "            best_topic = topic[0]\n",
    "    doc_topic_pair = (doc_num, best_topic, max_prob)\n",
    "    return(doc_topic_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4 \n",
    "##### Apply the above function to each document in our corpus by using a loop.\n",
    "##### What does the list of tuples contain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list of the same length as the number of documents.\n",
    "doc_topic_pairs_ex = [None]*dictionary_ex.num_docs\n",
    "\n",
    "# Loop through a range of document indices.\n",
    "for i in range(dictionary_ex.num_docs):\n",
    "    # For each document index, get the document-topic tuple.\n",
    "    doc_topic_pairs_ex[i] = GetDocTopicPair(i, corpus_tfidf_ex, lda_model_tfidf_ex)\n",
    "\n",
    "print(doc_topic_pairs_ex[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The list of tuples represents each document with it's best fit topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5 \n",
    "##### Create a dataframe called `doc_topic_df_ex` and assign the list of tuples to it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe out of a list of tuples.\n",
    "doc_topic_df_ex = pd.DataFrame(doc_topic_pairs_ex)\n",
    "# Assign column names to the dataframe.\n",
    "doc_topic_df_ex.columns = [\"doc_id\", \"best_topic\", \"best_probability\"]\n",
    "print(doc_topic_df_ex.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6 \n",
    "##### Retrieve all documents with word count less than 3 and assign original index from UN data to our `doc_topic_df_ex` dataframe.\n",
    "##### Print the last 5 rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of articles that we kept.\n",
    "valid_snippets_ex = np.where(word_counts_array_ex >= 3)[0]\n",
    "print(len(valid_snippets_ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now assign the index of the original article to be the index of the dataframe.\n",
    "doc_topic_df_ex.index = valid_snippets_ex\n",
    "print(doc_topic_df_ex.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7 \n",
    "##### Retrieve all documents assigned to topic 2. Save it in `topic2_docs` and output the top ten documents assigned to that topic.\n",
    "##### Print the number of documents assigned to topic 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and sort all documents assigned to topic 2 by probability in descending order.\n",
    "topic2_docs = doc_topic_df_ex.query(\"best_topic==1\")\n",
    "topic2_docs = topic2_docs.sort_values(by = \"best_probability\", ascending = False)\n",
    "print(topic2_docs.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many documents were assigned to that topic.\n",
    "print(topic2_docs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 8 \n",
    "##### Get the indices of the top 15 documents in the topic and then the headlines of the top 15 documents in the topic from the \n",
    "##### original UN dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the top 15 documents that were assigned to that topic.\n",
    "top_15 = topic2_docs.index[0:15,]\n",
    "# Inspect the top 15 documents in topic 2.\n",
    "UN_articles_topic2 = UN.loc[top_15, :]\n",
    "print(UN_articles_topic2[['title']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 9 \n",
    "##### Save the LDA visualization as a HTML file called `UN_LDAvis`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the plot as a self-contained HTML file.\n",
    "pyLDAvis.save_html(vis, plot_dir+\"/UN_LDAvis.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 \n",
    "##### Generate a TDM from corpus weighted with TF-IDF, name it `TDM_tf_idf_ex`\n",
    "##### Check the dimensions of the type of the TDM.\n",
    "##### How many terms and documents are there in the 2D array?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert corpus weighted with TF-IDF to a TDM matrix.\n",
    "TDM_tf_idf_ex = matutils.corpus2dense(corpus_tfidf_ex,\n",
    "                                      DTM_ex.shape[1],\n",
    "                                      DTM_ex.shape[0])\n",
    "\n",
    "\n",
    "print(type(TDM_tf_idf_ex))\n",
    "print(TDM_tf_idf_ex.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Convert the above TDM into a DTM called `DTM_tf_idf_ex`.\n",
    "##### Print the dimensions of the matrix.\n",
    "##### Save the matrix created as a dataframe called `DTM_df_ex`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose matrix to get the DTM.\n",
    "DTM_tf_idf_ex = TDM_tf_idf_ex.transpose()\n",
    "\n",
    "print(DTM_tf_idf_ex.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DTM weighted with TF-IDF.\n",
    "DTM_df_ex = pd.DataFrame(DTM_tf_idf_ex,\n",
    "                         columns = DTM_ex.columns, \n",
    "                         index = valid_snippets_ex) #<- set index to original article index\n",
    "print(DTM_df_ex.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3 \n",
    "##### Compute cosine similarity for the `DTM_tf_idf_ex` matrix.\n",
    "##### Print the shape of the matrix.\n",
    "##### Save the similarity matrix as a dataframe called `similarity_df_ex`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix (a numpy 2D array).\n",
    "similarity_ex = cosine_similarity(DTM_tf_idf_ex)\n",
    "print(type(similarity_ex))\n",
    "\n",
    "print(similarity_ex.shape)\n",
    "\n",
    "# Create similarity dataframe with appropriate column names and indices.\n",
    "similarity_df_ex = pd.DataFrame(similarity_ex,\n",
    "                                columns = valid_snippets_ex,\n",
    "                                index = valid_snippets_ex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
