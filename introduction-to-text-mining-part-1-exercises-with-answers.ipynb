{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Mining Part 1 - Exercises with Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Import the required packages.\n",
    "##### Set `main_dir` to the location of your `booz-allen-hamilton` folder.\n",
    "##### Make `data_dir` from the `main_dir` and concatenate remainder of the path to data directory.\n",
    "##### Set the working directory to `data_dir`.\n",
    "##### Check if the working directory is updated to `data_dir`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper packages.\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Packages with tools for text processing.\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Set `home_dir` to the root directory of your computer.\n",
    "home_dir = Path.home()\n",
    "\n",
    "# Set `main_dir` to the location of your `booz-allen-hamilton` folder.\n",
    "main_dir = home_dir / \"Documents\" / \"NLP_Intro\" / \"intro-to-text-mining-main\"\n",
    "data_dir = main_dir / \"Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amirmokhtari/Documents/NLP_Intro/intro-to-text-mining-main/data\n"
     ]
    }
   ],
   "source": [
    "# Change the working directory.\n",
    "os.chdir(data_dir)\n",
    "\n",
    "# Check the working directory.\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/amirmokhtari/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "#####  Load the corpus from `UN_agreement_titles.csv` into a new variable `agreements`.\n",
    "#####  Print the columns of `agreements`.\n",
    "#####  Print the first 5 rows and check the output to see if data is loaded correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'PosixPath' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d7d8e88eb8ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load corpus from a text document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magreements\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/UN_agreement_titles.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'PosixPath' and 'str'"
     ]
    }
   ],
   "source": [
    "# Load corpus from a text document\n",
    "agreements  = pd.read_csv(data_dir + '/UN_agreement_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agreements' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-612ad1af3831>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print the columns of `agreements`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magreements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'agreements' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the columns of `agreements`.\n",
    "print(agreements.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agreements' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9f9680bd48ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print the first 5 rows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magreements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'agreements' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the first 5 rows.\n",
    "print(agreements.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Make a series from the dataframe that contains only the `title` column of `agreements` and name it `titles`.\n",
    "##### Print the first 5 titles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a series from the dataframe, name it `titles`.\n",
    "titles = agreements[\"title\"]\n",
    "print(titles[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Tokenize each title in the series `titles` and assign it to `titles_tokenized`.\n",
    "##### Assign the first tokenized titles to `title_words` and print this out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: If you run into look-up error while using word_tokenize, `install punkt from nltk` using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each title into a large list of tokenized titles.\n",
    "titles_tokenized = [word_tokenize(titles[i]) for i in range(0,len(titles))]\n",
    "\n",
    "# First tokenized title.\n",
    "titles_words = titles_tokenized[0]\n",
    "print(titles_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Clean the `titles_words` in the following order:\n",
    "##### 1. Convert all characters to lower case and assign it to `titles_words`.\n",
    "##### 2. Remove stop words from `titles_words` and assign it to `titles_words`.\n",
    "##### 3. Remove punctuation, numbers, and all other symbols that are not letters of the alphabet \n",
    "#####    from `titles_words` and assign it to `titles_words`.\n",
    "##### 4. Stem words in `titles_words` and assign it to `titles_words`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: If you run into look-up error while using stopwords, `install stopwords from nltk` using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert to lower case.\n",
    "titles_words = [word.lower() for word in titles_words]\n",
    "print(titles_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove stop words.\n",
    "# Get common English stop words.\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words.\n",
    "titles_words = [word for word in titles_words if not word in stop_words]\n",
    "print(titles_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Remove punctuation and any non-alphabetical characters.\n",
    "titles_words = [word for word in titles_words if word.isalpha()]\n",
    "print(titles_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Stem words.\n",
    "titles_words = [PorterStemmer().stem(word) for word in titles_words]\n",
    "print(titles_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Create an empty list `titles_clean` whose length is same as `titles_tokenized`.\n",
    "##### Perform the above steps on the list `titles_tokenized` and also record the length of each title in 'word_counts_per_titles'.\n",
    "##### Check the first 10 words in 300th title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector for clean titles.\n",
    "titles_clean = [None] * len(titles_tokenized)\n",
    "\n",
    "# Create a vector of word counts for each clean titles.\n",
    "word_counts_per_titles = [None] * len(titles_tokenized)\n",
    "\n",
    "# Process words in all documents.\n",
    "for i in range(len(titles_tokenized)):\n",
    "    # 1. Convert to lower case.\n",
    "    titles_clean[i] = [titles.lower() for titles in titles_tokenized[i]]\n",
    "    \n",
    "    # 2. Remove stopwords.\n",
    "    titles_clean[i] = [word for word in titles_clean[i] if not word in stop_words]\n",
    "    \n",
    "    # 3. Remove punctuation and any non-alphabetical characters.\n",
    "    titles_clean[i] = [word for word in titles_clean[i] if word.isalpha()]\n",
    "    \n",
    "    # 4. Stem words.\n",
    "    titles_clean[i] = [PorterStemmer().stem(word) for word in titles_clean[i]]\n",
    "    \n",
    "    # Record the word count per titles.\n",
    "    word_counts_per_titles[i] = len(titles_clean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 10 words in 300th titles.\n",
    "# Index will be 299 since the first row has index of 0.\n",
    "print(titles_clean[299][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Print the first 10 rows of `word_counts_per_titles` .\n",
    "##### Plot a histogram for  `word_counts_per_titles`, and set bins to number of unique values in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at total word counts per title (for the first 10).\n",
    "print(word_counts_per_titles[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the histogram.\n",
    "plt.hist(word_counts_per_titles, bins = len(set(word_counts_per_titles)))\n",
    "plt.xlabel('Number of words per titles')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Convert word counts list and snippets list to numpy arrays named `ex_word_counts_array` and  `titles_array` and print the length of  `titles_array`.\n",
    "##### Find indices of all snippets where there are greater than or equal to 3 words and save it to `valid_titles`. Print length of `valid_titles`.\n",
    "##### Subset the `titles_array` to keep only those where there are at least 3 words. Print length of `titles_array`.\n",
    "##### Convert it back to a list `titles_clean`.  Print first 5 rows of `titles_clean`.\n",
    "##### Combine word tokens in each titles into a single string and save the result as a list called `titles_clean_list`. Print the first 5 titles in `titles_clean_list`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array with length of each titles.\n",
    "ex_word_counts_array = np.array(word_counts_per_titles)\n",
    "titles_array = np.array(titles_clean)\n",
    "print(len(titles_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of all messages where there are at least 3 words.\n",
    "valid_titles = np.where(ex_word_counts_array >= 3)[0]\n",
    "print(len(valid_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the titles array to keep only those where there are at least 3 words.\n",
    "titles_array = titles_array[valid_titles]\n",
    "print(len(titles_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the array back to a list.\n",
    "titles_clean = titles_array.tolist()\n",
    "print(titles_clean[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join words in each message into a single character string.\n",
    "titles_clean_list = [' '.join(message) for message in titles_clean]\n",
    "print(titles_clean_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Use the function we defined in class that takes a list of character strings and a name of an output file and writes it into a txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function.\n",
    "def write_lines(lines, filename):   #<- given lines to write and filename\n",
    "    joined_lines = '\\n'.join(lines) #<- join lines with line breaks\n",
    "    file = open(ex_out_filename, 'w')  #<- open write only file \n",
    "    file.write(joined_lines)        #<- write lines to file\n",
    "    file.close()                    #<- close connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save output file name to a variable `ex_out_filename` and call the text file \"ex_clean_titles.txt\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file name to a variable.\n",
    "ex_out_filename = \"ex_clean_titles.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write sequences to file.\n",
    "write_lines(titles_clean_list, ex_out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Create a `CountVectorizer()` and save it as `ex_vec`.\n",
    "##### Create a DTM of the `titles_clean_list` and name it `ex_X`.\n",
    "##### Convert `ex_X` to an array.\n",
    "##### Print the  first 20 feature names of `ex_vec`.\n",
    "##### Convert `ex_X` to a pandas dataframe `ex_DTM` and print the top 5 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_vec = CountVectorizer()\n",
    "ex_X = ex_vec.fit_transform(titles_clean_list)\n",
    "print(ex_X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex_vec.get_feature_names()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the matrix into a pandas dataframe for easier manipulation.\n",
    "ex_DTM = pd.DataFrame(ex_X.toarray(), columns = ex_vec.get_feature_names())\n",
    "print(ex_DTM.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Use the convenience function that sorts and looks at first n-entries in the dictionary we defined in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HeadDict(dict_x, n):\n",
    "    # Get items from the dictionary and sort them by\n",
    "    # value key in descending (i.e. reverse) order.\n",
    "    sorted_x = sorted(dict_x.items(),\n",
    "    reverse = True,\n",
    "    key = lambda kv: kv[1])\n",
    "    # Convert sorted dictionary to a list.\n",
    "    dict_x_list = list(sorted_x)\n",
    "    # Return the first `n` values from the dictionary only.\n",
    "    return(dict(dict_x_list[:n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Sum the counts of each word in all documents and save the series as a dictionary `ex_corpus_freq_dist`.\n",
    "##### Print the top 30 words and their counts in `ex_corpus_freq_dist`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum frequencies of each word in all documents.\n",
    "ex_DTM.sum(axis = 0).head()\n",
    "\n",
    "# Save series as a dictonary.\n",
    "ex_corpus_freq_dist = ex_DTM.sum(axis = 0).to_dict()\n",
    "\n",
    "# Glance at the top 30 words with highest counts.\n",
    "print(HeadDict(ex_corpus_freq_dist, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Save `ex_X`, `ex_DTM`, `ex_word_counts_array`, `valid_titles`, `titles_clean`,  `titles_clean_list` and `ex_corpus_freq_dist`  files as pickles `ex_DTM_matrix`, `ex_DTM`, `ex_word_counts_array`, `valid_titles`, `ex_titles_clean`, `ex_titles_clean_list` and `ex_corpus_freq_dist` to be used in the next module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ex_X, open('ex_DTM_matrix.sav', 'wb'))\n",
    "pickle.dump(ex_DTM, open('ex_DTM.sav', 'wb'))\n",
    "pickle.dump(ex_word_counts_array, open('ex_word_counts_array.sav', 'wb'))\n",
    "pickle.dump(valid_titles, open('valid_titles.sav', 'wb'))\n",
    "pickle.dump(titles_clean, open('ex_titles_clean.sav', 'wb'))\n",
    "pickle.dump(titles_clean_list, open('ex_titles_clean_list.sav', 'wb'))\n",
    "pickle.dump(ex_corpus_freq_dist, open('ex_corpus_freq_dist.sav', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
