{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Mining Part 2 - Exercises with Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 \n",
    "##### Load the libraries that are used in this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper packages.\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Packages with tools for text processing.\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Packages for working with text data.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Packages for getting data ready for and building a LDA model\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2 \n",
    "##### Set `main_dir` to the location of your `booz-allen-hamilton` folder.\n",
    "##### Make `data_dir` from the `main_dir` and concatenate remainder of the path to data directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Set `home_dir` to the root directory of your computer.\n",
    "home_dir = Path.home()\n",
    "\n",
    "# Set `main_dir` to the location of your `booz-allen-hamilton` folder.\n",
    "main_dir = home_dir / \"Desktop\" / \"booz-allen-hamilton\"\n",
    "\n",
    "# Make `data_dir` from the `main_dir` and remainder of the path to data directory.\n",
    "data_dir = main_dir / \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3 \n",
    "##### Set the working directory to `data_dir`.\n",
    "##### Check if the working directory is updated to `data_dir`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory.\n",
    "os.chdir(data_dir)\n",
    "\n",
    "# Check the working directory.\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4 \n",
    "##### Load the pickled files from the previous exercises: `ex_titles_clean.sav`, `ex_corpus_freq_dist` and `ex_titles_clean_list`.\n",
    "##### Save them as `processed_docs_ex`, `ex_corpus_freq_dist` and `titles_clean_list`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs_ex = pickle.load(open(\"ex_titles_clean.sav\",\"rb\"))\n",
    "ex_corpus_freq_dist = pickle.load(open(\"ex_corpus_freq_dist.sav\",\"rb\"))\n",
    "titles_clean_list = pickle.load(open(\"ex_titles_clean_list.sav\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Plot the frequency distribution of words in `ex_corpus_freq_dist` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a FreqDist object native to nltk.\n",
    "ex_corpus_freq_dist = nltk.FreqDist(ex_corpus_freq_dist)\n",
    "\n",
    "# Plot distribution for the entire corpus.\n",
    "plt.figure(figsize = (16, 7))\n",
    "ex_corpus_freq_dist.plot(80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Create a word cloud of the entire corpus and name it `ex_wordcloud`.\n",
    "##### Plot the wordcloud and set `figsize` to` (14, 7)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud from corpus.\n",
    "ex_wordcloud = WordCloud(max_font_size = 40, background_color = \"white\", collocations = False)\n",
    "ex_wordcloud = ex_wordcloud.generate(' '.join(titles_clean_list))\n",
    "\n",
    "# Plot the cloud using matplotlib.\n",
    "plt.figure(figsize = (14, 7))\n",
    "plt.imshow(ex_wordcloud, interpolation = \"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Check the length of `processed_docs_ex` using `len()` function.\n",
    "##### Create a dictionary from `processed_docs_ex` object, using `gensim.corpora.Dictionary` function.\n",
    "##### Label the dictionary `dictionary_ex`.\n",
    "##### Then loop through the dictionary printing out the first 10 items, including key and value.\n",
    "##### Make sure to set the seed as `2` for exercises.\n",
    "##### Use `.filter_extremes()` to filter items. Set `no_below` as `5`, `no_above` as `0.5` and `keep_n` as `942`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  To check the length, use `len(processed_docs_ex)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_docs_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed. \n",
    "np.random.seed(2)\n",
    "\n",
    "dictionary_ex = gensim.corpora.Dictionary(processed_docs_ex)\n",
    "\n",
    "# The loop below iterates through the first 10 items of the dictionary and prints out the key and value. \n",
    "count = 0\n",
    "for k, v in dictionary_ex.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "\n",
    "# Key stands for the order of the word within all the words in the corpus, words are in alphabetical order.\n",
    "# Value stands for the actual word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dictionary_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_ex.filter_extremes(no_below = 5, no_above = 0.5, keep_n = 942)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Write a list comprehension that transforms each doc within the `processed_docs_ex` .\n",
    "##### Save this object, the output of the list comprehension, as `bow_corpus_ex`.\n",
    "##### What type of object is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus_ex = [dictionary_ex.doc2bow(doc) for doc in processed_docs_ex]\n",
    "\n",
    "# What type of object is this?\n",
    "type(bow_corpus_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Transform `bow_corpus_ex` to a TF-IDF transformed object using `TfIdfModel()`.\n",
    "##### Name the object as `corpus_tfidf_ex`.\n",
    "##### Preview the scores for the first document in `corpus_tfidf_ex` using `pprint`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the transformation.\n",
    "tfidf = models.TfidfModel(bow_corpus_ex)\n",
    "\n",
    "# Apply the transformation to the entire corpus.\n",
    "corpus_tfidf_ex = tfidf[bow_corpus_ex]\n",
    "\n",
    "# Preview TF-IDF scores for the first document.\n",
    "for doc in corpus_tfidf_ex:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Pickle `bow_corpus_ex`, `corpus_tfidf_ex` and `dictionary_ex` for the next session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_corpus_ex, open('bow_corpus_ex.sav', 'wb'))\n",
    "pickle.dump(corpus_tfidf_ex, open('corpus_tfidf_ex.sav', 'wb'))\n",
    "pickle.dump(dictionary_ex, open('dictionary_ex.sav', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
