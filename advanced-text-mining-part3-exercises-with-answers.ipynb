{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Text Mining Part 3 - Exercises with answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Load libraries that are used in this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper packages.\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Packages for loading pre-trained word-embedding model\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2\n",
    "##### Set `main_dir` to the location of your `booz-allen-hamilton` folder.\n",
    "##### Make `data_dir` from the `main_dir` and concatenate remainder of the path to data directory.\n",
    "##### Make `plots_dir` from the `main_dir` and concatenate remainder of the path to plots directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Set `home_dir` to the root directory of your computer.\n",
    "home_dir = Path.home()\n",
    "\n",
    "# Set `main_dir` to the location of your `booz-allen-hamilton` folder.\n",
    "main_dir = home_dir / \"Desktop\" / \"booz-allen-hamilton\"\n",
    "\n",
    "# Make `data_dir` from the `main_dir` and remainder of the path to data directory.\n",
    "data_dir = main_dir / \"data\"\n",
    "\n",
    "# Make `plots_dir` from the `main_dir` and remainder of the path to plots directory.\n",
    "plot_dir = main_dir / \"plots\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3 \n",
    "##### Set the working directory to `data_dir`.\n",
    "##### Check if the working directory is updated to `data_dir`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory.\n",
    "os.chdir(data_dir)\n",
    "# Check the working directory.\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "#####  Load the corpus from `UN_agreement_titles.csv` into a new variable `agreements`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus from a text document.\n",
    "agreements  = pd.read_csv(data_dir + '/UN_agreement_titles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Make a series from the dataframe that contains only the `title` column of `agreements` and name it `titles`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a series from the dataframe, name it `titles`.\n",
    "titles = agreements[\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Tokenize each title in the series `titles` and assign it to `ex_titles_tokenized`.\n",
    "##### Assign the first tokenized titles to `ex_title_words` and print this out.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each title into a large list of tokenized titles.\n",
    "ex_titles_tokenized = [word_tokenize(titles[i]) for i in range(0,len(titles))]\n",
    "\n",
    "# First tokenized title.\n",
    "ex_titles_words = ex_titles_tokenized[0]\n",
    "print(ex_titles_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "##### Define and run a cleaning function to convert to lower case, remove stop words, remove punctuation and any non-alphabetical characters on the list `ex_titles_tokenized` and return `ex_titles_clean_list` and `ex_titles_clean`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(ex_titles_clean, ex_titles_tokenized):\n",
    "\n",
    "# Process words in all documents.\n",
    "    for i in range(len(ex_titles_tokenized)):\n",
    "    # 1. Convert to lower case.\n",
    "        ex_titles_clean[i] = [titles.lower() for titles in ex_titles_tokenized[i]]\n",
    "    \n",
    "    # 2. Remove stopwords.\n",
    "        ex_titles_clean[i] = [word for word in ex_titles_clean[i] if not word in stop_words]\n",
    "    \n",
    "    # 3. Remove punctuation and any non-alphabetical characters.\n",
    "        ex_titles_clean[i] = [word for word in ex_titles_clean[i] if word.isalpha()]\n",
    "    \n",
    "    ex_titles_clean_list = [' '.join(snippet) for snippet in ex_titles_clean]\n",
    "    return ex_titles_clean_list, ex_titles_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 8\n",
    "##### Create an empty list `ex_titles_clean_not_stemmed` for clean titles whose length is same as `ex_titles_tokenized` \n",
    "##### Clean tokens for each title in `ex_titles_clean_list` using the cleaning function\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector for clean titles.\n",
    "ex_titles_clean_not_stemmed = [None] * len(ex_titles_tokenized)\n",
    "\n",
    "ex_titles_clean_list,ex_titles_clean = cleaning(ex_titles_clean_not_stemmed,ex_titles_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 9\n",
    "##### Initialize `CountVectorizer`\n",
    "##### Transform the list of titles into DTM and show output as a matrix\n",
    "##### Convert the matrix into a pandas dataframe for easier manipulation and print the top rows of the dataframe\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "\n",
    "X = vec.fit_transform(ex_titles_clean_list)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_DTM_not_stemmed = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\n",
    "print(ex_DTM_not_stemmed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 10\n",
    "\n",
    "##### Using `ex_titles_clean`, create a `Word2Vec` model and name as `ex_model`. \n",
    "##### Be sure to use the same parameters as we did in the module. \n",
    "\n",
    "##### Print the `vector_size` of `ex_model`. \n",
    "##### Also, just like we did in the module, see what similar words come up for `administration` and `united` in this model. \n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_model = Word2Vec(ex_titles_clean, \n",
    "                 size = 200, \n",
    "                 min_count = 3,\n",
    "                 iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex_model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex_model.wv.most_similar('administration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex_model.wv.most_similar('united'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "#### Task 1\n",
    "\n",
    "##### Load the pre-trained glove embeddings and save as `glove_file`. \n",
    "##### We will be loading the file with vector size of 200. \n",
    "\n",
    "##### Define `LoadGloveModel()` function as we did in class to extract workd embeddings from the glove file. \n",
    "##### Save the outputs from `LoadGloveModel` function as `ex_glove_model`. \n",
    "\n",
    "#### Result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of glove dimensions.\n",
    "GLOVE_DIM = 200\n",
    "\n",
    "# Load pre-trained glove embeddings.\n",
    "glove_file = data_dir + \"/glove.6B.200d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Use f = open(glove_file,'r', encoding= 'utf-8') if unicode errors occur\n",
    "\n",
    "def LoadGloveModel(glove_file):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(glove_file,'r',encoding=\"utf8\")\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\", len(model), \" words loaded!\")\n",
    "    return model\n",
    "\n",
    "# Load embedings from file.\n",
    "ex_glove_model = LoadGloveModel(glove_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Check the first few embeddings of `ex_glove_model`. \n",
    "##### Create a frequency count of each word in the corpus using `ex_DTM_not_stemmed`  and save it to `ex_corpus_freq_dist`.\n",
    "##### Save `ex_corpus_freq_dist` as a dataframe named `ex_word_counts`.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(list(ex_glove_model.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save series as a dictionary.\n",
    "ex_corpus_freq_dist = ex_DTM_not_stemmed.sum(axis = 0).to_dict()\n",
    "dict(list(ex_corpus_freq_dist.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word counts for exploratory analysis.\n",
    "ex_word_counts = pd.DataFrame(list(ex_corpus_freq_dist.items()), columns = ['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex_word_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Iniitialize the following variables as shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings matrix.\n",
    "ex_DICT_SIZE = len(ex_word_counts.index)\n",
    "ex_word_emb_matrix = np.zeros((ex_DICT_SIZE, GLOVE_DIM))\n",
    "ex_words = list(ex_word_counts.word)\n",
    "ex_NUM_MESSAGES = len(ex_titles_clean_not_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a loop to extract the vectors from `glove_model` and save to `ex_word_emb_matrix`.\n",
    "##### Print its shape and the first vector.\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(ex_DICT_SIZE):\n",
    "    w = ex_words[i]\n",
    "    vect = ex_glove_model.get(w)\n",
    "\n",
    "    if vect is not None:\n",
    "        ex_word_emb_matrix[i] = vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex_word_emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex_word_emb_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Convert `ex_DTM_not_stemmed` to a  numpy array.\n",
    "##### Compute sums of all word counts for each tweet and save as `ex_DTM_row_sums`,\n",
    "##### Create `titles_embeddings_matrix` by multiplying `ex_DTM_non_stemmed` with `ex_word_emb_matrix`.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to a numpy array\n",
    "ex_DTM_not_stemmed = ex_DTM_not_stemmed.to_numpy()\n",
    "\n",
    "# Compute sums of all word counts for each chat message\n",
    "ex_DTM_row_sums = np.sum(ex_DTM_not_stemmed, axis=1)\n",
    "\n",
    "titles_embeddings_matrix = ex_DTM_not_stemmed.dot(ex_word_emb_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Compute the weighted average of each document by using a loop to average `titles_embeddings_matrix` using `ex_DTM_row_sums`.\n",
    "##### Save `titles_embeddings_matrix` as a dataframe named `titles_emb_df` and print the results.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(ex_NUM_MESSAGES):\n",
    "    titles_embeddings_matrix[i] = np.true_divide(titles_embeddings_matrix[i], ex_DTM_row_sums[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a data frame and add chat message ids.\n",
    "titles_emb_df = pd.DataFrame(titles_embeddings_matrix)\n",
    "print(titles_emb_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Take a look at the first title (`titles[0]`)\n",
    "##### Convert its vector representation in `titles_emb_df` and save to `target_titles_emb`. Print the first 5 results.\n",
    "* Note: You can use `pd.set_option('display.max_colwidth', -1)` and `pd.set_option('display.max_rows', 2000)` to see the whole (non-truncated) message in a column of a dataframe or in a series.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "\n",
    "titles[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Average embeddingstitles = titles_df['titles'] \n",
    "target_titles_emb = titles_emb_df.loc[0].to_numpy()\n",
    "print(target_titles_emb[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Find cosine similarity `ex_similarity_scores` for `titles_emb_df` and `target_titles_emb`. Don't forget to reshape `target_titles_emb`.\n",
    "##### Convert `ex_similarity_scores` into a dataframe named `ex_similarity_scores_df`. Set index as `titles_df.index`.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_similarity_scores = cosine_similarity(titles_emb_df, target_titles_emb.reshape(1, -1))\n",
    "ex_similarity_scores[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ex_similarity_scores_df = pd.DataFrame(ex_similarity_scores, \n",
    "                                    columns = ['similarity_score'],\n",
    "                                    index = titles.index)\n",
    "print(ex_similarity_scores_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Sort values of `ex_similarity_scores_df` in descending order.\n",
    "##### Print the first 3 most similar documents to the target document.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_similarity_scores_df.sort_values('similarity_score', ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(titles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(titles[371])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(titles[609])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Plot a histogram to see the distribution of cosine similarity scores using the similarity scores from `ex_similarity_scores_df`.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results.\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "cm = plt.cm.PRGn\n",
    "n, bins, patches = plt.hist(ex_similarity_scores_df['similarity_score'], \n",
    "                            20, color='green')\n",
    "for i, p in enumerate(patches):\n",
    "    plt.setp(p, 'facecolor', cm(i/25)) # notice the i/25\n",
    "fig.suptitle('Distribution of Cosine Similarity Scores', fontsize=20)\n",
    "plt.xlabel(\"Similarity score\", fontsize=18)\n",
    "plt.ylabel(\"Number of titles\", fontsize=18)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
