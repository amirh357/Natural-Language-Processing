{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Text Mining Part 3 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Load libraries that are used in this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2\n",
    "##### Set `main_dir` to the location of your `booz-allen-hamilton` folder.\n",
    "##### Make `data_dir` from the `main_dir` and concatenate remainder of the path to data directory.\n",
    "##### Make `plots_dir` from the `main_dir` and concatenate remainder of the path to plots directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3 \n",
    "##### Set the working directory to `data_dir`.\n",
    "##### Check if the working directory is updated to `data_dir`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "#####  Load the corpus from `UN_agreement_titles.csv` into a new variable `agreements`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Make a series from the dataframe that contains only the `title` column of `agreements` and name it `titles`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Tokenize each title in the series `titles` and assign it to `ex_titles_tokenized`.\n",
    "##### Assign the first tokenized titles to `ex_title_words` and print this out.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "##### Define and run a cleaning function to convert to lower case, remove stop words, remove punctuation and any non-alphabetical characters on the list `ex_titles_tokenized` and return `ex_titles_clean_list` and `ex_titles_clean`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 8\n",
    "##### Create an empty list `ex_titles_clean_not_stemmed` for clean titles whose length is same as `ex_titles_tokenized` \n",
    "##### Clean tokens for each title in `ex_titles_clean_list` using the cleaning function\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 9\n",
    "##### Initialize `CountVectorizer`\n",
    "##### Transform the list of titles into DTM and show output as a matrix\n",
    "##### Convert the matrix into a pandas dataframe for easier manipulation and print the top rows of the dataframe\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 10\n",
    "\n",
    "##### Using `ex_titles_clean`, create a `Word2Vec` model and name as `ex_model`. \n",
    "##### Be sure to use the same parameters as we did in the module. \n",
    "\n",
    "##### Print the `vector_size` of `ex_model`. \n",
    "##### Also, just like we did in the module, see what similar words come up for `administration` and `united` in this model. \n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "#### Task 1\n",
    "\n",
    "##### Load the pre-trained glove embeddings and save as `glove_file`. \n",
    "##### We will be loading the file with vector size of 200. \n",
    "\n",
    "##### Define `LoadGloveModel()` function as we did in class to extract workd embeddings from the glove file. \n",
    "##### Save the outputs from `LoadGloveModel` function as `ex_glove_model`. \n",
    "\n",
    "#### Result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Check the first few embeddings of `ex_glove_model`. \n",
    "##### Create a frequency count of each word in the corpus using `ex_DTM_not_stemmed`  and save it to `ex_corpus_freq_dist`.\n",
    "##### Save `ex_corpus_freq_dist` as a dataframe named `ex_word_counts`.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Iniitialize the following variables as shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings matrix.\n",
    "ex_DICT_SIZE = len(ex_word_counts.index)\n",
    "ex_word_emb_matrix = np.zeros((ex_DICT_SIZE, GLOVE_DIM))\n",
    "ex_words = list(ex_word_counts.word)\n",
    "ex_NUM_MESSAGES = len(ex_titles_clean_not_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a loop to extract the vectors from `glove_model` and save to `ex_word_emb_matrix`.\n",
    "##### Print its shape and the first vector.\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Convert `ex_DTM_not_stemmed` to a  numpy array.\n",
    "##### Compute sums of all word counts for each tweet and save as `ex_DTM_row_sums`,\n",
    "##### Create `titles_embeddings_matrix` by multiplying `ex_DTM_non_stemmed` with `ex_word_emb_matrix`.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Compute the weighted average of each document by using a loop to average `titles_embeddings_matrix` using `ex_DTM_row_sums`.\n",
    "##### Save `titles_embeddings_matrix` as a dataframe named `titles_emb_df` and print the results.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Take a look at the first title (`titles[0]`)\n",
    "##### Convert its vector representation in `titles_emb_df` and save to `target_titles_emb`. Print the first 5 results.\n",
    "* Note: You can use `pd.set_option('display.max_colwidth', -1)` and `pd.set_option('display.max_rows', 2000)` to see the whole (non-truncated) message in a column of a dataframe or in a series.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Find cosine similarity `ex_similarity_scores` for `titles_emb_df` and `target_titles_emb`. Don't forget to reshape `target_titles_emb`.\n",
    "##### Convert `ex_similarity_scores` into a dataframe named `ex_similarity_scores_df`. Set index as `titles_df.index`.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Sort values of `ex_similarity_scores_df` in descending order.\n",
    "##### Print the first 3 most similar documents to the target document.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Plot a histogram to see the distribution of cosine similarity scores using the similarity scores from `ex_similarity_scores_df`.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
